{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sage.spelling_correction import (\n",
    "    T5ModelForSpellingCorruption,\n",
    "    RuM2M100ModelForSpellingCorrection,\n",
    "    AvailableCorrectors,\n",
    ")\n",
    "from datetime import datetime\n",
    "\n",
    "corrector = T5ModelForSpellingCorruption.from_pretrained(\n",
    "    AvailableCorrectors.ent5_large.value\n",
    ")\n",
    "\n",
    "corrector.model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "\"\"\"Optimization for T5 model\"\"\"\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers.models.t5.configuration_t5 import T5Config\n",
    "from transformers.models.auto.modeling_auto import (\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    ")\n",
    "from transformers.models.t5.modeling_t5 import T5Attention, T5ForConditionalGeneration\n",
    "\n",
    "from fastseq.logging import get_logger\n",
    "from fastseq.utils.api_decorator import replace\n",
    "\n",
    "logger = get_logger(__name__, logging.INFO)\n",
    "\n",
    "\n",
    "@replace(T5Attention)\n",
    "class T5AttentionV2(T5Attention):\n",
    "    \"\"\"Optimized T5Attention for self-attn and encoder-decoder-attn in T5.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: T5Config,\n",
    "        has_relative_attention_bias=False,\n",
    "        num_beams=1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            config=config,\n",
    "            has_relative_attention_bias=has_relative_attention_bias,\n",
    "        )\n",
    "        self.num_beams = num_beams\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        mask=None,\n",
    "        key_value_states=None,\n",
    "        position_bias=None,\n",
    "        past_key_value=None,\n",
    "        layer_head_mask=None,\n",
    "        query_length=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n",
    "        \"\"\"\n",
    "        # Input is (batch_size, seq_length, dim)\n",
    "        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n",
    "        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n",
    "        batch_size, seq_length = hidden_states.shape[:2]\n",
    "\n",
    "        real_seq_length = seq_length\n",
    "\n",
    "        is_encoder_decoder_attn = key_value_states is not None\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            assert (\n",
    "                len(past_key_value) == 2\n",
    "            ), f\"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states\"\n",
    "            real_seq_length += (\n",
    "                past_key_value[0].shape[2] if query_length is None else query_length\n",
    "            )\n",
    "\n",
    "        key_length = (\n",
    "            real_seq_length if key_value_states is None else key_value_states.shape[1]\n",
    "        )\n",
    "\n",
    "        def shape(states):\n",
    "            \"\"\"projection\"\"\"\n",
    "            return states.view(\n",
    "                batch_size, -1, self.n_heads, self.key_value_proj_dim\n",
    "            ).transpose(1, 2)\n",
    "\n",
    "        def unshape(states):\n",
    "            \"\"\"reshape\"\"\"\n",
    "            return (\n",
    "                states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n",
    "            )\n",
    "\n",
    "        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n",
    "            \"\"\"projects hidden states correctly to key/query states\"\"\"\n",
    "            if key_value_states is None:\n",
    "                # self-attn\n",
    "                # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "                hidden_states = shape(proj_layer(hidden_states))\n",
    "            elif past_key_value is None:\n",
    "                # cross-attn\n",
    "                # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "                hidden_states = shape(proj_layer(key_value_states))\n",
    "\n",
    "            if past_key_value is not None:\n",
    "                if key_value_states is None:\n",
    "                    # self-attn\n",
    "                    # (batch_size, n_heads, key_length, dim_per_head)\n",
    "                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n",
    "                else:\n",
    "                    # cross-attn\n",
    "                    hidden_states = past_key_value\n",
    "            return hidden_states\n",
    "\n",
    "        # get query states\n",
    "        query_states = shape(\n",
    "            self.q(hidden_states)\n",
    "        )  # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "\n",
    "        # get key/value states\n",
    "        key_states = project(\n",
    "            hidden_states,\n",
    "            self.k,\n",
    "            key_value_states,\n",
    "            past_key_value[0] if past_key_value is not None else None,\n",
    "        )\n",
    "        value_states = project(\n",
    "            hidden_states,\n",
    "            self.v,\n",
    "            key_value_states,\n",
    "            past_key_value[1] if past_key_value is not None else None,\n",
    "        )\n",
    "\n",
    "        if self.is_decoder and use_cache is True:\n",
    "            if is_encoder_decoder_attn:\n",
    "                if past_key_value is None:\n",
    "                    key_states = key_states.view(\n",
    "                        batch_size // self.num_beams,\n",
    "                        self.num_beams,\n",
    "                        self.n_heads,\n",
    "                        key_length,\n",
    "                        self.key_value_proj_dim,\n",
    "                    )[:, 0:1, :, :, :].contiguous()\n",
    "                    value_states = value_states.view(\n",
    "                        batch_size // self.num_beams,\n",
    "                        self.num_beams,\n",
    "                        self.n_heads,\n",
    "                        key_length,\n",
    "                        self.key_value_proj_dim,\n",
    "                    )[:, 0:1, :, :, :].contiguous()\n",
    "            present_key_value_state = (key_states, value_states)\n",
    "        else:\n",
    "            present_key_value_state = None\n",
    "\n",
    "        if is_encoder_decoder_attn and use_cache:\n",
    "            new_query_states = query_states.view(\n",
    "                batch_size // self.num_beams,\n",
    "                self.num_beams,\n",
    "                self.n_heads,\n",
    "                seq_length,\n",
    "                self.key_value_proj_dim,\n",
    "            )\n",
    "            scores = torch.einsum(\n",
    "                \"bmnqd,bxnkd->bmnqk\", new_query_states, key_states\n",
    "            ).reshape(\n",
    "                -1, self.n_heads, seq_length, key_length\n",
    "            )  # (bs, n_heads, qlen, klen)\n",
    "        else:\n",
    "            scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
    "            # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n",
    "\n",
    "        if position_bias is None:\n",
    "            if not self.has_relative_attention_bias:\n",
    "                position_bias = torch.zeros(\n",
    "                    (1, self.n_heads, real_seq_length, key_length),\n",
    "                    device=scores.device,\n",
    "                    dtype=scores.dtype,\n",
    "                )\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "                    position_bias.requires_grad = True\n",
    "            else:\n",
    "                position_bias = self.compute_bias(real_seq_length, key_length)\n",
    "\n",
    "            # if key and values are already calculated\n",
    "            # we want only the last query position bias\n",
    "            if past_key_value is not None:\n",
    "                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n",
    "\n",
    "            if mask is not None:\n",
    "                position_bias = (\n",
    "                    position_bias + mask\n",
    "                )  # (batch_size, n_heads, seq_length, key_length)\n",
    "\n",
    "        scores = scores + position_bias\n",
    "        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n",
    "            scores\n",
    "        )  # (batch_size, n_heads, seq_length, key_length)\n",
    "        attn_weights = nn.functional.dropout(\n",
    "            attn_weights, p=self.dropout, training=self.training\n",
    "        )  # (batch_size, n_heads, seq_length, key_length)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if layer_head_mask is not None:\n",
    "            attn_weights = attn_weights * layer_head_mask\n",
    "\n",
    "        if is_encoder_decoder_attn and use_cache:\n",
    "            tmp_weights = attn_weights.view(\n",
    "                batch_size // self.num_beams,\n",
    "                self.num_beams,\n",
    "                self.n_heads,\n",
    "                seq_length,\n",
    "                key_length,\n",
    "            )\n",
    "            attn_output = torch.einsum(\n",
    "                \"bmnqk,bxnkd->bmnqd\", tmp_weights, value_states\n",
    "            ).reshape(\n",
    "                -1, self.n_heads, seq_length, self.key_value_proj_dim\n",
    "            )  # (bs, n_heads, qlen, dim_per_head)\n",
    "        else:\n",
    "            attn_output = torch.matmul(\n",
    "                attn_weights, value_states\n",
    "            )  # (bs, n_heads, qlen, dim_per_head)\n",
    "        attn_output = unshape(attn_output)  # (bs, qlen, dim)\n",
    "\n",
    "        attn_output = self.o(attn_output)\n",
    "\n",
    "        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (attn_weights,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "@replace(T5ForConditionalGeneration)\n",
    "class T5ForConditionalGenerationV2(T5ForConditionalGeneration):\n",
    "    \"\"\"Optimized T5ForConditionalGenerationV2\"\"\"\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        # if decoder past is not included in output\n",
    "        # speedy decoding is disabled and no need to reorder\n",
    "        print(past, beam_idx)\n",
    "        if past is None:\n",
    "            logger.warning(\n",
    "                \"You might want to consider setting `use_cache=True` to speed up decoding\"\n",
    "            )\n",
    "            return past\n",
    "\n",
    "        reordered_decoder_past = ()\n",
    "        for layer_past_states in past:\n",
    "            # get the correct batch idx from layer past batch dim\n",
    "            # batch dim of `past` is at 2nd position\n",
    "            reordered_layer_past_states = ()\n",
    "            for layer_past_state in layer_past_states[0:2]:\n",
    "                # need to set correct `past` for each of the four key / value states\n",
    "                reordered_layer_past_states = reordered_layer_past_states + (\n",
    "                    layer_past_state.index_select(\n",
    "                        0, beam_idx.to(layer_past_state.device)\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "            reordered_layer_past_states = (\n",
    "                reordered_layer_past_states + layer_past_states[2:]\n",
    "            )\n",
    "\n",
    "            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n",
    "            assert len(reordered_layer_past_states) == len(layer_past_states)\n",
    "\n",
    "            reordered_decoder_past = reordered_decoder_past + (\n",
    "                reordered_layer_past_states,\n",
    "            )\n",
    "        return reordered_decoder_past\n",
    "\n",
    "\n",
    "MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING[T5Config] = (\n",
    "    T5ForConditionalGenerationV2  # pylint: disable=line-too-long\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrector.model = T5ForConditionalGenerationV2.from_pretrained(\n",
    "    AvailableCorrectors.ent5_large.value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrector.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrector.model.to(\"cuda:3\")\n",
    "corrector.model.eval()\n",
    "\n",
    "start = datetime.now()\n",
    "metrics = corrector.evaluate(\n",
    "    \"t5_optimization/libs/sage/data/example_data/jfleg\",\n",
    "    batch_size=16,\n",
    "    prefix=\"grammar: \",\n",
    "    # size=3,\n",
    ")\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(duration)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0:18:21.561730\n",
    "{'Precision': 83.39, 'Recall': 84.25, 'F1': 83.82} - batch 1\n",
    "\n",
    "0:02:45.232933\n",
    "{'Precision': 83.39, 'Recall': 84.25, 'F1': 83.82} - batch 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrector.model.to(\"cuda:3\")\n",
    "# corrector.model.eval()\n",
    "\n",
    "start = datetime.now()\n",
    "metrics = corrector.evaluate(\n",
    "    \"t5_optimization/libs/sage/data/example_data/jfleg\",\n",
    "    batch_size=16,\n",
    "    prefix=\"grammar: \",\n",
    "    # size=3,\n",
    "    use_cache=True,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(duration)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0:19:08.841463\n",
    "{'Precision': 83.39, 'Recall': 84.25, 'F1': 83.82} - batch 1\n",
    "\n",
    "0:02:44.470877\n",
    "{'Precision': 83.39, 'Recall': 84.25, 'F1': 83.82} - batch 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итог: медленнее оригинала. Кеширование не работает. Вероятно потому что фрейм устарел или параметры с которыми генерирую я не совсем подходят."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
