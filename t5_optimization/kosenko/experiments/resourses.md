- [whisper.cpp](https://github.com/ggerganov/whisper.cpp)
- [gpt-fast](https://github.com/pytorch-labs/gpt-fast)
- [EAGLE](https://github.com/SafeAILab/EAGLE)
- [koboldcpp](https://github.com/LostRuins/koboldcpp)
- [speculative-decoding](https://github.com/lucidrains/speculative-decoding)
- [ts_server](https://bellard.org/ts_server/)
- [fastT5](https://github.com/Ki6an/fastT5) - работает с некоторыми исправлениями старого апи.
- [Neural Network Compression: Techniques for Reducing Size and ImprovingLatency](https://youtu.be/-QCbDjpIM2I?si=1Zv6-RxcuilKWXIl)
- [https://vgel.me/posts/faster-inference/](https://vgel.me/posts/faster-inference/)
- [UMIA-Group/FourierTransformer: The official Pytorch implementation of the paper "Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator" (ACL 2023 Findings)](https://github.com/LUMIA-Group/FourierTransformer)
- [Neural Network Distiller by Intel AI Lab: a Python package for neural network compression research.](https://github.com/IntelLabs/distiller/)
- [Exploring Extreme Parameter Compression for Pre-trained Language Models](https://arxiv.org/pdf/2205.10036.pdf)
- [EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference](https://arxiv.org/pdf/2011.14203.pdf)
- [https://github.com/ELS-RD/kernl/tree/main](https://github.com/ELS-RD/kernl/tree/main) - зависает на большой модели, не заработало.
- [Fast & Simple repository for pre-training and fine-tuning T5-style models](https://github.com/PiotrNawrot/nanoT5)
- [Fused Softmax triton](https://triton-lang.org/main/getting-started/tutorials/02-fused-softmax.html)
- [PyTorch compile to speed up inference on Llama 2](https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/)
- [Fuse Modules Recipe](https://pytorch.org/tutorials/recipes/fuse.html)
- [Dynamic Quantization pytorch](https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html) - хорошо работает
- [Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)
- [Static Quantization with Eager Mode in PyTorch](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)
- [QUANTIZATION pytorch](https://pytorch.org/docs/stable/quantization.html)
- [Quantized Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html)
- [Pytorch Mobile Performance Recipes](https://pytorch.org/tutorials/recipes/mobile_perf.html)
- [A BetterTransformer for Fast Transformer Inference](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)
- [fastseq An efficient implementation of the popular sequence models for text generation, translation tasks.](https://github.com/microsoft/fastseq) - не получилось получить никакого прироста на целевой задаче. требует дополнительного изучения.
- [Accelerating HuggingFace T5 Inference with TensorRT](https://github.com/NVIDIA/TensorRT/blob/release/8.2/demo/HuggingFace/notebooks/t5.ipynb)
- [TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs)](https://github.com/NVIDIA/TensorRT-LLM/tree/main)
- [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models](https://pytorch.org/docs/main/torch.compiler_aot_inductor.html)
- [PYTORCH MOBILE End-to-end workflow from Training to Deployment for iOS and Android mobile devices](https://pytorch.org/mobile/home/)
- [An easy to use PyTorch to TensorRT converter](https://github.com/NVIDIA-AI-IOT/torch2trt)
- [Lite Transformer with Long-Short Range Attention](https://github.com/mit-han-lab/lite-transformer)
- [EdgeTran: Co-designing Transformers for Efficient Inference on Mobile Edge Platforms](https://arxiv.org/pdf/2303.13745.pdf)
- [MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND MOBILE-FRIENDLY VISION TRANSFORMER](https://arxiv.org/pdf/2110.02178.pdf)
- [EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction](https://hanlab.mit.edu/projects/efficientvit)
- [Efficient AI Inference & Serving](https://github.com/hpcaitech/SwiftInfer)
- [[ICLR 2024] Efficient Streaming Language Models with Attention Sinks](https://github.com/mit-han-lab/streaming-llm)
- [TinyChat: Efficient and Lightweight Chatbot with AWQ](https://github.com/mit-han-lab/llm-awq/tree/main/tinychat) 
- [PockEngine: Sparse and Efficient Fine-tuning in a Pocket](https://hanlab.mit.edu/projects/pockengine)
- [Tiny Machine Learning: Progress and Futures](https://hanlab.mit.edu/projects/tiny-machine-learning-progress-and-futures)
- [Offsite-Tuning: Transfer Learning without Full Model](https://github.com/mit-han-lab/offsite-tuning)
- [[CVPR'23] FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer](https://github.com/mit-han-lab/flatformer)
- [On-Device Training Under 256KB Memory [NeurIPS'22]](https://github.com/mit-han-lab/tiny-training)

### courses
- [TinyML and Efficient Deep Learning Computing](https://hanlab.mit.edu/courses/2023-fall-65940)
