{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не заработало. Зависает и ничего не делает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ELS-RD/kernl/blob/main/tutorial/t5%20e2e.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/user-name-goes-here/.local/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sage.spelling_correction import (\n",
    "    RuM2M100ModelForSpellingCorrection,\n",
    "    AvailableCorrectors,\n",
    ")\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"API to T5-based models for spelling correction.\n",
    "\n",
    "To load a model:\n",
    "\n",
    "    from corrector import AvailableCorrectors\n",
    "\n",
    "    model = T5ModelForSpellingCorruption.from_pretrained(AvailableCorrectors.fred_large.value)\n",
    "    ...\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Optional, Union, Any\n",
    "from tqdm import tqdm\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "from sage.spelling_correction.corrector import Corrector\n",
    "\n",
    "\n",
    "class T5ModelForSpellingCorruption(Corrector):\n",
    "    \"\"\"T5-based models.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name_or_path: Union[str, os.PathLike]):\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.max_model_length = 512\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name_or_path: Union[str, os.PathLike]):\n",
    "        engine = cls(model_name_or_path)\n",
    "        engine.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            model_name_or_path,\n",
    "        )\n",
    "        engine.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name_or_path, eos_token=\"</s>\"\n",
    "        )\n",
    "\n",
    "        return engine\n",
    "\n",
    "    def batch_correct(\n",
    "        self,\n",
    "        sentences: List[str],\n",
    "        batch_size: int,\n",
    "        prefix: Optional[str] = \"\",\n",
    "    ) -> List[List[Any]]:\n",
    "        \"\"\"Correct multiple sentences\"\"\"\n",
    "\n",
    "        batches = [\n",
    "            sentences[i : i + batch_size] for i in range(0, len(sentences), batch_size)\n",
    "        ]\n",
    "        result = []\n",
    "        # pb = tqdm(total=len(batches))\n",
    "        # device = self.model.device\n",
    "        for batch in batches:\n",
    "            batch_prefix = [prefix + sentence for sentence in batch]\n",
    "            batch_prefix = batch_prefix[0]\n",
    "            encodings = self.tokenizer(\n",
    "                batch_prefix,\n",
    "                pad_to_multiple_of=8,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(\"cuda\")\n",
    "            with torch.inference_mode(), torch.autocast(\n",
    "                dtype=torch.float32, cache_enabled=True, device_type=\"cuda\"\n",
    "            ):\n",
    "                generated_tokens = self.model.generate(\n",
    "                    **encodings,\n",
    "                    max_length=encodings[\"input_ids\"].shape[1] + 1,\n",
    "                )\n",
    "            ans = self.tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            result.append(ans)\n",
    "            # pb.update(1)\n",
    "        return result\n",
    "\n",
    "\n",
    "corrector = T5ModelForSpellingCorruption.from_pretrained(\n",
    "    AvailableCorrectors.ent5_large.value\n",
    ")\n",
    "\n",
    "corrector.model.to(torch.device(\"cuda\"))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1602 [00:00<00:09, 169.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 80.0, 'Recall': 80.0, 'F1': 80.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sage.evaluation.evaluate import evaluation\n",
    "\n",
    "dataset_name_or_path = \"t5_optimization/sage/data/example_data/jfleg\"\n",
    "src_file = open(os.path.join(dataset_name_or_path, \"sources.txt\"))\n",
    "corr_file = open(os.path.join(dataset_name_or_path, \"corrections.txt\"))\n",
    "sources = src_file.read().split(\"\\n\")\n",
    "corrections = corr_file.read().split(\"\\n\")\n",
    "\n",
    "size = 3\n",
    "if size == -1:\n",
    "    size = len(sources)\n",
    "batch_size = 1\n",
    "prefix = \"grammar: \"\n",
    "answers = corrector.batch_correct(\n",
    "    sources[:size],\n",
    "    batch_size,\n",
    "    prefix,\n",
    ")\n",
    "answers = sum(answers, [])\n",
    "metrics = evaluation(sources, corrections, answers)\n",
    "print(metrics)\n",
    "# answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo as torchdynamo\n",
    "from kernl.model_optimization import optimize_model\n",
    "\n",
    "optimize_model(corrector.model.encoder)\n",
    "optimize_model(corrector.model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sage.evaluation.evaluate import evaluation\n",
    "\n",
    "\n",
    "dataset_name_or_path = \"t5_optimization/sage/data/example_data/jfleg\"\n",
    "src_file = open(os.path.join(dataset_name_or_path, \"sources.txt\"))\n",
    "corr_file = open(os.path.join(dataset_name_or_path, \"corrections.txt\"))\n",
    "sources = src_file.read().split(\"\\n\")\n",
    "corrections = corr_file.read().split(\"\\n\")\n",
    "\n",
    "size = 10\n",
    "if size == -1:\n",
    "    size = len(sources)\n",
    "batch_size = 1\n",
    "prefix = \"grammar: \"\n",
    "answers = corrector.batch_correct(\n",
    "    sources[:size],\n",
    "    batch_size,\n",
    "    prefix,\n",
    ")\n",
    "answers = sum(answers, [])\n",
    "metrics = evaluation(sources, corrections, answers)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo as torchdynamo\n",
    "from kernl.model_optimization import optimize_model\n",
    "\n",
    "torchdynamo.config.cache_size_limit = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_model(corrector.model.encoder)\n",
    "optimize_model(corrector.model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrector.model.to(torch.float16)\n",
    "# corrector.model.eval()\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrector.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': 91.67, 'Recall': 73.33, 'F1': 81.48}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode(), torch.autocast(\n",
    "    dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"\n",
    "):\n",
    "\n",
    "    start = datetime.now()\n",
    "\n",
    "    metrics = corrector.evaluate(\n",
    "\n",
    "        \"t5_optimization/sage/data/example_data/jfleg\",\n",
    "        batch_size=1,\n",
    "        prefix=\"grammar: \",\n",
    "    )\n",
    "\n",
    "\n",
    "    duration = datetime.now() - start\n",
    "    print(duration)\n",
    "\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "выдает ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = corrector.tokenizer(\n",
    "    \"translate English to French: The house in the woods is wonderful, can we buy it ?\",\n",
    "    return_tensors=\"pt\",\n",
    "    pad_to_multiple_of=8,\n",
    "    padding=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.inference_mode(), torch.autocast(\n",
    "    dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"\n",
    "):\n",
    "    # for _ in range(10):\n",
    "    #     output = corrector.model.generate(\n",
    "    #         inputs=input_ids[\"input_ids\"],\n",
    "    #         min_length=22,\n",
    "    #         max_length=22,\n",
    "    #     )\n",
    "    output = corrector.model.generate(\n",
    "        inputs=input_ids[\"input_ids\"],\n",
    "        # min_length=22,\n",
    "        # max_length=22,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### official version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 5.79MB/s]\n",
      "model.safetensors: 100%|██████████| 242M/242M [00:01<00:00, 128MB/s]  \n",
      "generation_config.json: 100%|██████████| 147/147 [00:00<00:00, 490kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 12.0MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.82MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 12.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16454567096661776\n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n",
      "205.29581649205647\n",
      "0.06526946800295264\n",
      "2.5x speedup\n",
      "La maison dans les bois est merveilleuse, pouvons-nous l'acheter? \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import time\n",
    "import torch._dynamo as torchdynamo\n",
    "import torch\n",
    "from kernl.model_optimization import optimize_model\n",
    "\n",
    "# default cache size needs to be increased to store the many graphs with generative models\n",
    "torchdynamo.config.cache_size_limit = 512\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.eval().cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"translate English to French: The house in the woods is wonderful, can we buy it ?\",\n",
    "    return_tensors=\"pt\",\n",
    "    pad_to_multiple_of=8,\n",
    "    padding=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.inference_mode(), torch.autocast(\n",
    "    dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"\n",
    "):\n",
    "    for _ in range(10):\n",
    "        output = model.generate(\n",
    "            inputs=input_ids[\"input_ids\"],\n",
    "            min_length=22,\n",
    "            max_length=22,\n",
    "        )\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    output = model.generate(\n",
    "        inputs=input_ids[\"input_ids\"],\n",
    "        min_length=22,\n",
    "        max_length=22,\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "    latency_baseline = time.perf_counter() - start\n",
    "    print(latency_baseline)\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "optimize_model(model.encoder)\n",
    "optimize_model(model.decoder)\n",
    "\n",
    "\n",
    "# warmup (IRL, encoder and decoder should be warmed each on their own)\n",
    "with torch.inference_mode(), torch.autocast(\n",
    "    dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"\n",
    "):\n",
    "    start = time.perf_counter()\n",
    "    model.generate(inputs=input_ids[\"input_ids\"], min_length=22, max_length=22)\n",
    "    print(time.perf_counter() - start)\n",
    "\n",
    "\n",
    "with torch.inference_mode(), torch.autocast(\n",
    "    dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"\n",
    "):\n",
    "    for _ in range(10):\n",
    "        model.generate(\n",
    "            inputs=input_ids[\"input_ids\"],\n",
    "            min_length=22,\n",
    "            max_length=22,\n",
    "        )\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    output = model.generate(\n",
    "        inputs=input_ids[\"input_ids\"],\n",
    "        min_length=22,\n",
    "        max_length=22,\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "    latency_optimized = time.perf_counter() - start\n",
    "    print(latency_optimized)\n",
    "    print(f\"{latency_baseline/latency_optimized:.1f}x speedup\")\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode(), torch.autocast(\n",
    "    dtype=torch.float16, cache_enabled=True, device_type=\"cuda\"\n",
    "):\n",
    "\n",
    "    for _ in range(10):\n",
    "        model.generate(\n",
    "\n",
    "            inputs=input_ids[\"input_ids\"],\n",
    "            min_length=22,\n",
    "\n",
    "            max_length=22,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
