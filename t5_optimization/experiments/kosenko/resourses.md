- [whisper.cpp](https://github.com/ggerganov/whisper.cpp)
- [gpt-fast](https://github.com/pytorch-labs/gpt-fast)
- [EAGLE](https://github.com/SafeAILab/EAGLE)
- [koboldcpp](https://github.com/LostRuins/koboldcpp)
- [speculative-decoding](https://github.com/lucidrains/speculative-decoding)
- [ts_server](https://bellard.org/ts_server/)
- [fastT5](https://github.com/Ki6an/fastT5)
- [Neural Network Compression: Techniques for Reducing Size and ImprovingLatency](https://youtu.be/-QCbDjpIM2I?si=1Zv6-RxcuilKWXIl)
- [https://vgel.me/posts/faster-inference/](https://vgel.me/posts/faster-inference/)
- [UMIA-Group/FourierTransformer: The official Pytorch implementation of the paper "Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator" (ACL 2023 Findings)](https://github.com/LUMIA-Group/FourierTransformer)
- [Neural Network Distiller by Intel AI Lab: a Python package for neural network compression research.](https://github.com/IntelLabs/distiller/)
- [Exploring Extreme Parameter Compression for Pre-trained Language Models](https://arxiv.org/pdf/2205.10036.pdf)
- [EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference](https://arxiv.org/pdf/2011.14203.pdf)
- [https://github.com/ELS-RD/kernl/tree/main](https://github.com/ELS-RD/kernl/tree/main)
- 
